{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9-Fre4FMda"
   },
   "source": [
    "###### **Data Streaming Application (CN7030) CRWK 20-21 [30 marks]**\n",
    "# **Group ID: Group 6*\n",
    "1.   Student 1: Pavithra and u2060586\n",
    "2.   Student 2: Pratikshaben Hasmukhbhai VYAS and u2048815\n",
    "\n",
    "---\n",
    "\n",
    "If you want to add comments on your group work, please write it here for us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdMZR-9QTwG3"
   },
   "source": [
    "# **Initiate and Configure Spark Streaming [5 marks]**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_z0p88Xtw_3-"
   },
   "outputs": [],
   "source": [
    "#Instalation and initiallization of spark\n",
    "# Load Spark engine\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initiate spark\n",
    "spark = SparkSession.builder.appName(\"Group6_StructuredStreamingOnStreaming data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and representing the stream of input lines\n",
    "linesDS = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 7000).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(linesDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHft1Jht1Qxl"
   },
   "source": [
    "\n",
    "# **Task - Data Streaming using PySpark [25 marks]**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark streaming:\n",
    "\n",
    "Spark streaming is done on streaming data. \n",
    "Streaming data is the data which gets continuously updated,\n",
    "the challenges faced on working with the streaming data are it requires 2 layers,\n",
    "1. Storage layer and 2. Processing layer.\n",
    "In this the data needs to get processed sequentially and incrementally on the real-time data \n",
    "which gets updated for every sec or millisecond. \n",
    "The data will get generated from one or many sources, \n",
    "which will be keep sending the data record simultaneously for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Structured streaming:\n",
    "In structured streaming, data streaming is same as like batch computation on the static data.\n",
    "It is scalable and fault tolerant. \n",
    "Structure streaming is built on the spark sql engine. \n",
    "In which spar sql engine perform the computation or function incrementally \n",
    "and will update the result of streaming data continuously when it arrives.\n",
    "\n",
    "* Concept- \n",
    "here take input Streaming data as input table. \n",
    "To this, every data which arrives will gets to added newly as a new row and will get appended to the input table. \n",
    "Further, query applied on the input table will generate a result table.\n",
    "With every time interval, new rows will get added to input table and this will also update the result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting each and every word\n",
    "wordsplit = linesDS.select(explode(split(linesDS.value,\" \")).alias(\"wordsplit\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers from the Sream Data\n",
    "num_remove = wordsplit.select(regexp_replace(\"wordsplit\",'\\d','').alias('NumberRemoval'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[NumberRemoval: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(num_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing symbols and white space from stream data\n",
    "SymAndSpaceremove = num_remove.select(regexp_replace(\"NumberRemoval\",'[\\W\\s]','').alias('SymbolRemoval'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SymbolRemoval: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(SymAndSpaceremove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates from stream data\n",
    "#DuplicateRemove=SymAndSpaceremove.select(regexp_replace('SymbolRemoval', '([^,]+)(,\\1)+(,|$)', '\\1\\3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits words in stream data by length\n",
    "lengthcount = SymAndSpaceremove.select(trim(length('SymbolRemoval')).alias(\"Length\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Length: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(lengthcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping each word from stream data by its length\n",
    "Result = lengthcount.groupBy(\"Length\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 modes of output, complete, append, update. In which we have used complete mode.\n",
    "\n",
    "* Complete mode:\n",
    "\n",
    "The complete updated result will be displayed, \n",
    "in which the whole updated result will be written on the external storage\n",
    "\n",
    "\n",
    "* Append mode:\n",
    "\n",
    "In this it will display only the result for the newly appended rows on the result table, \n",
    "the previous data result or last trigger will be written only on the external storage.\n",
    "\n",
    "\n",
    "* Update mode:\n",
    "\n",
    "In this method it will display only the newly appended rows on the result table and\n",
    "also the same will be written on the external storage too. \n",
    "This mode will write only the output  achieved from the last trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query compilation and displaying the output through console.\n",
    "Output_d = Result.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "#complete - keeps all the count from the first\n",
    "#append - leaves the previous output and keep counting the newly added one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_d.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIM6uLApSxi2"
   },
   "source": [
    "# **Convert ipynb to HTML for Turnitin submission**\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Lx9-Fre4FMda"
   ],
   "name": "Your_Group_ID_Streaming_CN7030.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
